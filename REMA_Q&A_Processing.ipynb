{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubamakabra/MagnusBox/blob/main/REMA_Q%26A_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aMOiu22xWWx"
      },
      "source": [
        "## Run this and then restart the runtime. There is some funk with the shapely installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8vM92J2xUzU"
      },
      "outputs": [],
      "source": [
        "!pip install -U google-cloud-aiplatform \"shapely<2\"\n",
        "!pip install google-cloud-aiplatform==1.25.0\n",
        "!pip install langchain==0.0.187\n",
        "#!pip install xmltodict==0.13.0\n",
        "#!pip install unstructured==0.7.0 # used by langchain\n",
        "#!pip install pdf2image==1.16.3 #used by langchain\n",
        "#!pip install requests==2.31.0\n",
        "!pip install beautifulsoup4==4.12.2\n",
        "!pip install chromadb\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZTMlH8c_SBz"
      },
      "source": [
        "## ChromaDB vector database for context + VertexAI response generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "f_-4_uWkv3tR",
        "outputId": "2406647a-0261-4fed-bfb2-fdb91bd34ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a28f367-d573-4925-b16a-63675cb7ab9b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a28f367-d573-4925-b16a-63675cb7ab9b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving next-poc-400210-640e2c5a3f5d.json to next-poc-400210-640e2c5a3f5d.json\n",
            "Activated service account credentials for: [magnus@next-poc-400210.iam.gserviceaccount.com]\n",
            "Updated property [core/account].\n",
            "gs://next-poc-bucket/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project next-poc-400210\n",
        "from google.colab import files\n",
        "\n",
        "upoaded = files.upload()\n",
        "!gcloud auth activate-service-account --key-file=next-poc-400210-640e2c5a3f5d.json\n",
        "!gcloud config set account magnus@next-poc-400210.iam.gserviceaccount.com\n",
        "!gsutil ls\n",
        "\n",
        "PROJECT_ID = 'next-poc-400210'\n",
        "REGION = 'us-central1'\n",
        "BUCKET = 'gs://next-poc-bucket/REMA/vectors'\n",
        "DISPLAY_NAME='REMA-document-answering'\n",
        "TEXT_GENERATION_MODEL='text-bison@001'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now we need to upload the JSON object we want to embed"
      ],
      "metadata": {
        "id": "1Ivjkyr1Ypx8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW0u45d0__UW"
      },
      "outputs": [],
      "source": [
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEQvSGLlv8qz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "json_path = 'recipes.json'\n",
        "\n",
        "with open(json_path, 'r') as file:\n",
        "    recipesjson = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwABndjWFU0T"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# This is likely reduntant, and might need a polish\n",
        "formatted_json_string = json.dumps(json.loads(recipesjson))\n",
        "json_object = json.loads(formatted_json_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_1uQaEP3-jx"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "import pandas as pd\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"chroma_vectors\")\n",
        "collection = client.get_or_create_collection(name = 'REMA-document-embeddings', embedding_function = 'paraphrase-MiniLM-L3-v2')\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L3-v2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#client.delete_collection(name='REMA-document-embeddings')"
      ],
      "metadata": {
        "id": "F8_eMgWReWxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFeYWrnRIpjN"
      },
      "outputs": [],
      "source": [
        "docs = []\n",
        "embeds = []\n",
        "metas = []\n",
        "indexes = []\n",
        "index = 0\n",
        "\n",
        "for doc in json_object:\n",
        "\n",
        "    docs.append(str(doc))\n",
        "    doc_id = f\"doc_{index}\"\n",
        "    metadata = {\n",
        "        \"document_id\": doc_id,\n",
        "        \"cookMins\": doc['cookMins'],\n",
        "        \"difficulty\": doc['difficulty']\n",
        "        }\n",
        "    metas.append(metadata)\n",
        "\n",
        "    embedding = model.encode(str(doc)).tolist()\n",
        "    embeds.append(embedding)\n",
        "\n",
        "    indexes.append(str(index))\n",
        "    index = index + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY0hM0j5GxLt"
      },
      "outputs": [],
      "source": [
        "collection.add(\n",
        "    documents = docs,\n",
        "    metadatas = metas,\n",
        "    embeddings = embeds,\n",
        "    ids = indexes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWJ-hVkJ6EGb"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "from langchain.vectorstores.matching_engine import MatchingEngine\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.agents import Tool\n",
        "from google.cloud import aiplatform\n",
        "import os\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "def matching_engine_search(question):\n",
        "    model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
        "    input = model.encode(question).tolist()\n",
        "\n",
        "    relevant_documentation = collection.query(\n",
        "        query_embeddings=[input],\n",
        "        n_results=8\n",
        "    )\n",
        "    context = relevant_documentation['documents']\n",
        "\n",
        "    return str(context)\n",
        "\n",
        "def query_the_machine(question):\n",
        "  matching_engine_response=matching_engine_search(question)\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Follow exactly these 3 steps:\n",
        "  1. Read the context below and aggregate this data\n",
        "  Context : {matching_engine_response}\n",
        "  2. Create the answer from only the context\n",
        "  3. Make a summary list of the ingredients\n",
        "  4. Make a bulleted summary of the instructions\n",
        "  5. Show the source for your answers\n",
        "  User Question: {question}\n",
        "\n",
        "  If you do not have any context or are unsure of the answer, reply that you do not know about this topic.\n",
        "  \"\"\"\n",
        "\n",
        "  prompt2 = f\"\"\"\n",
        "  Recipe Recommendation:\n",
        "  You have a rich database of diverse recipes at your disposal. These recipes vary in flavors, origins, and dietary benefits.\n",
        "  Your goal is to provide users with personalized meal suggestions based on their preferences and queries.\n",
        "  Context: {matching_engine_response}\n",
        "  User Query: {question}\n",
        "  Your task is to generate a recipe suggestion that aligns with the user's needs.\n",
        "  Each recipe you provide should include a brief summary of the dish, its cultural or regional origin,\n",
        "  any dietary information (e.g., vegan, protein-rich), a list of ingredients with quantities, and step-by-step cooking instructions.\n",
        "  You can respond with recipe suggestions and provide as much detail as possible, making the meal experience\n",
        "  enjoyable and satisfying for the user. Feel free to engage in a conversation and explore a world of culinary delights. Let's get cooking!\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  model = TextGenerationModel.from_pretrained(TEXT_GENERATION_MODEL)\n",
        "  response = model.predict(\n",
        "              prompt2,\n",
        "              temperature = 0.2,\n",
        "              top_k = 40,\n",
        "              top_p = 0.8,\n",
        "              max_output_tokens = 1024,\n",
        "  )\n",
        "\n",
        "  print(f\"Question: \\n{question}\")\n",
        "  print(f\"Response: \\n{response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT-K9EXe6T1U",
        "outputId": "e23662fc-c013-4f11-bc2a-a974f26b00ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the question : \n",
            "\n",
            "Suggest a meal that contains milk and meat\n",
            "Question: \n",
            "Suggest a meal that contains milk and meat\n",
            "Response: \n",
            "Sure, here is a recipe for a creamy pasta dish that contains milk and meat:\n",
            "\n",
            "**Ingredients:**\n",
            "\n",
            "* 1 pound penne pasta\n",
            "* 1 pound ground beef\n",
            "* 1 onion, chopped\n",
            "* 2 cloves garlic, minced\n",
            "* 1 (28 ounce) can crushed tomatoes\n",
            "* 1 (15 ounce) can tomato sauce\n",
            "* 1 cup heavy cream\n",
            "* 1/2 cup grated Parmesan cheese\n",
            "* 1/2 teaspoon dried oregano\n",
            "* 1/2 teaspoon salt\n",
            "* 1/4 teaspoon black pepper\n",
            "\n",
            "**Instructions:**\n",
            "\n",
            "1. In a large pot of boiling salted water, cook the pasta according to the package directions.\n",
            "2. While the pasta is cooking, brown the ground beef in a large skillet over medium heat. Drain the excess fat.\n",
            "3. Add the onion and garlic to the skillet and cook until softened, about 5 minutes.\n",
            "4. Stir in the crushed tomatoes, tomato sauce, heavy cream, Parmesan cheese, oregano, salt, and pepper. Bring to a simmer and cook for 15 minutes, or until the sauce has thickened.\n",
            "5. Drain the pasta and add it to the sauce. Stir to combine.\n",
            "6. Serve immediately, topped with additional Parmesan cheese.\n",
            "\n",
            "This dish is a hearty and satisfying meal that is perfect for a family dinner. It is also a great way to use up leftover pasta and ground beef.\n"
          ]
        }
      ],
      "source": [
        "question = \"Suggest a meal that contains milk and meat\"\n",
        "query_the_machine(question)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"\", \"\", \"\"]\n",
        "for q in questions:\n",
        "  query_the_machine(q)"
      ],
      "metadata": {
        "id": "_yXrrZykXhpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd1Z-k6YyYVu"
      },
      "outputs": [],
      "source": [
        "!gsutil cp -r chroma_vectors gs://next-poc-bucket/REMA/vectors"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLvNs5fwi9dQOSN4xn4/TE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}